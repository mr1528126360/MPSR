{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "data_1 = pd.read_csv(\"naocuzchuli1.csv\",sep=\",\", engine='python', encoding='utf-8')\n",
    "data = pd.read_csv(\"数据标注.csv\",sep=\",\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>姓名</th>\n",
       "      <th>ID</th>\n",
       "      <th>Lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>吴华好</td>\n",
       "      <td>MR201802210163-Wu HuaHao</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>梁郁庆</td>\n",
       "      <td>MR201706120215-LiangYuQing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>梁其盼</td>\n",
       "      <td>MR201602290243-Liang QiPan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>黄甜弟</td>\n",
       "      <td>MR201803090487-HuangTianDi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>黄江泉</td>\n",
       "      <td>MR201710230147-HuangJiangQuan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>林振洪</td>\n",
       "      <td>MR201804230415-LinZhenHong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>王陵</td>\n",
       "      <td>MR201804130045-WangLing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>林秀容</td>\n",
       "      <td>MR201804200221-LinXiuRong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>温景成</td>\n",
       "      <td>MR201804210230-WenJingCheng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>黄作松</td>\n",
       "      <td>MR201709140208-HuangZuoSong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      姓名                             ID  Lable\n",
       "0    吴华好       MR201802210163-Wu HuaHao      1\n",
       "1    梁郁庆     MR201706120215-LiangYuQing      1\n",
       "2    梁其盼     MR201602290243-Liang QiPan      1\n",
       "3    黄甜弟     MR201803090487-HuangTianDi      1\n",
       "4    黄江泉  MR201710230147-HuangJiangQuan      1\n",
       "..   ...                            ...    ...\n",
       "187  林振洪     MR201804230415-LinZhenHong      0\n",
       "188   王陵        MR201804130045-WangLing      0\n",
       "189  林秀容      MR201804200221-LinXiuRong      0\n",
       "190  温景成    MR201804210230-WenJingCheng      0\n",
       "191  黄作松    MR201709140208-HuangZuoSong      0\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR201802210163-Wu HuaHao\n",
      "MR201706120215-LiangYuQing\n",
      "MR201602290243-Liang QiPan\n",
      "MR201803090487-HuangTianDi\n",
      "MR201710230147-HuangJiangQuan\n",
      "MR2017062901390-LinGen You\n",
      "MR201707060266-ChenGuiYing\n",
      "MR201806250211-HuangHuiZhong\n",
      "MR201806040279-Li JinRong\n",
      "MR202103160080-Chen LianDing\n",
      "MR201807080094-He Zhong\n",
      "MR201708080222-Li GuiFeng\n",
      "MR202109160378-Liang YouDi\n",
      "MR201802200016-ZhaoDongSheng\n",
      "MR201803140073-Fang LiChang\n",
      "MR201711160167-Huang ZhiRong\n",
      "MR201709170104-LiangSuZhen\n",
      "MR202110280279-Yang XiTian\n",
      "MR201803170300-Huang GenFa\n",
      "MR201805060354-XieShengYu\n",
      "MR201812090066-Lin Nong\n",
      "MR201810030079-Huang QiDing\n",
      "MR201806070184-ZhouLiBo\n",
      "MR201811040118-Pan YiYou\n",
      "MR202204270343-Huang BingQiang\n",
      "MR201807130152-Zhang SheXie\n",
      "MR201901140071-Xiao BaoXi\n",
      "MR202007040281-Yin CaiDi\n",
      "MR201907170393-Ou FuTai\n",
      "MR201808280243-Lin ShiMei\n",
      "MR201808150286-Huang ZhengLian\n",
      "MR201902140144-Zhou RuiQiu\n",
      "MR201905270177-Lu SongMao\n",
      "MR201908300389-Huang YanChun\n",
      "MR201909240217-Huang XianYou\n",
      "MR201809280461-Huang GuiPing\n",
      "MR201908140375-Huang ShunAn\n",
      "MR201901010047-Zhao PanShun\n",
      "MR202203140317-Huang MingZhong\n",
      "MR201906120420-Zhang YanHua\n",
      "MR202204250318-Yang NuLan\n",
      "MR201811220371-Kuang JianHua\n",
      "MR201811010032-Zhao QiSheng\n",
      "MR201811170295-Kuang GuoHong\n",
      "MR201911230367-Zhou QunDuo\n",
      "MR202011170463-Zhou YunNu\n",
      "MR201812290239-Zhang ZhuoSen\n",
      "MR201906110314-Song YuCheng\n",
      "MR201901190185-Li DuSheng\n",
      "MR202111040024-Lin GuiMei\n",
      "MR202004090247-Liang DieRong\n",
      "MR201903030032-Zhou WenXing\n",
      "MR201910280291-LiLinYang\n",
      "MR201905210129-Peng DaZe\n",
      "MR201910010051-Zeng ShunFa\n",
      "MR201904300236-He ChangLian\n",
      "MR202012140342-Liang MeiShun\n",
      "MR202103270245-Zhou YunHao\n",
      "MR201909300333-Wu HuanLin\n",
      "MR201905280271-Zhang RenDe\n",
      "MR201911230290-Guo FengMan\n",
      "MR-Pan BaoCai\n",
      "MR201709050086-Mei BenZhou\n",
      "MR201709060196-XieJian\n",
      "MR201709050212-Hu ShengYin\n",
      "MR202003070164-He QuanHuan\n",
      "MR201709100131-LiangJinDi\n",
      "MR201709120100-LiangJinYuan\n",
      "MR201709080242-ZhuYuanTian\n",
      "MR201709110124-Zhou JianLe\n",
      "MR201709150146-He LanZhong\n",
      "MR201709180125-ChenShaoLing\n",
      "MR201709130216-LuNiuXi\n",
      "MR201709180311-LuoSan\n",
      "MR201709170194-WuChangYang\n",
      "MR201709250187-WeiLongXi\n",
      "MR201709250264-Zhang HuanJiao\n",
      "MR201709240112-LiaoSheSheng\n",
      "MR201709250212-OuYangLiJuan\n",
      "MR201709260229-Zhang MinHui\n",
      "MR201709290106-Liang DaFu\n",
      "MR201710030170-WangPeiGen\n",
      "MR201609020118-Zhou Wei\n",
      "MR201710080077-ZengGuoCai\n",
      "MR201710260143-LuZhi\n",
      "MR201710230298-LiangYunHao\n",
      "MR201710280069-HuangJiHan\n",
      "MR201710290080-ZhuJianSheng\n",
      "MR201711010144-ChenZhaoGuang\n",
      "MR201803070303-Mo JinMei\n",
      "MR201711040248-YangFengJiao\n",
      "MR201711090081-ZouSanMei\n",
      "MR201704290123-RenZiFeng\n",
      "MR201711080214-LiangBaiSheng\n",
      "MR201710190322-ZhaoDongQiang\n",
      "MR201711070313-Zhao YeQin\n",
      "MR201811160271-Li ZhaoQun\n",
      "MR201711200279-RongGuangNing\n",
      "MR201711220152-He MeiQun\n",
      "MR201711240273-ChenSiReng\n",
      "MR201711180209-GuoZhenQian\n",
      "MR201709100154-Lin ZhenQuan\n",
      "MR201711150153-KuangXingYing\n",
      "MR201711300156-LeFaZi\n",
      "MR201712010104-Chen EnHua\n",
      "MR201711290250-YaoYanRong\n",
      "MR201904240211-Wu YouGen\n",
      "MR201910070088-Ou DaYing\n",
      "MR201712010181-Liao LianZhong\n",
      "MR201712050082-FengZaiYing\n",
      "MR201709240102-Lao NuYong\n",
      "MR201712120209-Kuang YanPing\n",
      "MR201712170069-ZhangSanDing\n",
      "MR201712180097-Zhao LeSheng\n",
      "MR201712190182-WuXuShan\n",
      "MR201712220318-MaGuiXiang\n",
      "MR201712170079-WuWeiHuan\n",
      "MR201712210191-DengXiuJuan\n",
      "MR201712210375-XuQiNu\n",
      "MR201712240170-Liu QingXiao\n",
      "MR201712250442-Luo FuJin\n",
      "MR201712260308-ZhouJunQiang\n",
      "MR201712190202-ChenKunZhen\n",
      "MR201712260400-Liang JinKuan\n",
      "MR201712220354-LuoHuaJin\n",
      "MR201712310040-HuangYanZhuo\n",
      "MR201712260397-Huang GuoDong\n",
      "MR201801030282-LiChengQing\n",
      "MR201712260409-HuangYingYou\n",
      "MR201801270027-KuangGuoCai\n",
      "MR201805130099-PanBaoLiang\n",
      "MR201902120162-Zhang Cong\n",
      "MR201810280134-Rong MeiHuan\n",
      "MR201901181085-Hu YunLan\n",
      "MR201712310157-Zhou BaiSen\n",
      "MR201608140234-Liang JianRong\n",
      "MR201801130182-XuShengQuan\n",
      "MR201801060242-Wang SuZhen\n",
      "MR201908220397-Zhang GuoYin\n",
      "MR201801070152-ZhuShuiZhen\n",
      "MR201801040143-LiuCuiHua\n",
      "MR201802100202-KuangLiQin\n",
      "MR-Huang XinPei\n",
      "MR201801100184-KuangZhuMing\n",
      "MR201801250133-GanHongYing\n",
      "MR201801220266-ChenJianJun\n",
      "MR201801270108-Kuang ChunMei\n",
      "MR201801280085-ZhangChengJing\n",
      "MR201807100283-WuYuLing\n",
      "MR201802180035-ChenGuiQuan\n",
      "MR201910260239-Huang BaoSheng\n",
      "MR201802060103-YeChangYan\n",
      "MR201801260243-DengDongYi\n",
      "MR201802010201-HuangChengGang\n",
      "MR201811290275-Zhang MingMin\n",
      "MR201802120177-Huang ZhongTing\n",
      "MR201808150161-Sun Jing\n",
      "MR201803010795-HuangNuChang\n",
      "MR201802260271-WangXiuZhi\n",
      "MR201812140342-Lai ShuiYin\n",
      "MR201803080295-Liang JinPei\n",
      "MR201803080324-WuSongHua\n",
      "MR201803080017-LiuJian\n",
      "MR201803120307-Liu XiuLian\n",
      "MR201811230369-Luo PingMei\n",
      "MR201803080200-HuQunYing\n",
      "MR201803120328-ZhangXingLai\n",
      "MR201803150112-LiangChuanLiu\n",
      "MR201803120367-Zhou XiuNong\n",
      "MR201803140023-ChenYuHong\n",
      "MR201804220167-FengMei\n",
      "MR202106100062-Zhou XianMei\n",
      "MR201803300207-LuoXiaNu\n",
      "MR201812120201-Kuang YuXiong\n",
      "MR201803290560-ChenJianFang\n",
      "MR201804040022-Chen NianJin\n",
      "MR201803290614-HuangLe\n",
      "MR201708110264-LvYueQing\n",
      "MR201804130221-He WeiYuan\n",
      "MR201804090321-LiGuangMing\n",
      "MR201804130132-WuChangMei\n",
      "MR201808060137-Yang XianYing\n",
      "MR201903120286-Duan ShaoPing\n",
      "MR201804140320-HuangYuZhen\n",
      "MR201804200348-YiJiYuan\n",
      "MR201804170212-ChenWangMing\n",
      "MR201903010218-Lv RuiYing\n",
      "MR201804230415-LinZhenHong\n",
      "MR201804130045-WangLing\n",
      "MR201804200221-LinXiuRong\n",
      "MR201804210230-WenJingCheng\n",
      "MR201709140208-HuangZuoSong\n"
     ]
    }
   ],
   "source": [
    "all_names = []\n",
    "path_label=[]\n",
    "for h in range(0,len(data[\"ID\"])):\n",
    "    file_dir=data[\"ID\"][h]\n",
    "    print(file_dir)\n",
    "    all_names.append(file_dir)\n",
    "    path_label.append(data[\"Lable\"][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_list = all_names                  # all video file names\n",
    "all_y_list = np.array(path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_name_path = './UCF101actions.pkl'\n",
    "save_model_path = \"./ResNetCRNN_ckpt/\"\n",
    "data_path = \"../data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.0       # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 2             # number of target category\n",
    "epochs = 5       # training epochs\n",
    "batch_size = 144   #这要设置成和训练数据集一样的常度\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 18, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "def normalize_list(lst):\n",
    "    np_list = np.array(lst)\n",
    "    normalized_list = (np_list - np_list.min()) / (np_list.max() - np_list.min()) + 1\n",
    "    return normalized_list.tolist()\n",
    "def L1_tensor(x_feature):\n",
    "    for i in range(0, len(x_feature)):\n",
    "        #print(x_feature[i].shape)\n",
    "        list_1 = []\n",
    "        ls = x_feature[i]\n",
    "        for j in ls:\n",
    "            cc = np.sum(j)\n",
    "            cc = abs(cc)\n",
    "            list_1.append(cc)\n",
    "            # print(cc)\n",
    "        list_2 = normalize_list(list_1)\n",
    "        # print(list_2)\n",
    "        for k in range(0, len(x_feature[i])):\n",
    "            x_feature[i][k] = x_feature[i][k] * list_2[k]\n",
    "    return  x_feature\n",
    "if torch.cuda.is_available():\n",
    "    transformer_encoder = transformer_encoder.to('cuda')\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()       \n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation_2(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out_1.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "    \n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation_3(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out_1.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 2, 'pin_memory': True} if use_cuda else {'batch_size': batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split\n",
    "train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.25, random_state=42)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "train_set, valid_set = Dataset_CRNN(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_set_2, valid_set_2 = Dataset_CRNN_2(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN_2(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_set_3, valid_set_3 = Dataset_CRNN_3(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN_3(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)\n",
    "\n",
    "train_loader_2 = data.DataLoader(train_set_2, **params)\n",
    "valid_loader_2 = data.DataLoader(valid_set_2, **params)\n",
    "\n",
    "train_loader_3 = data.DataLoader(train_set_3, **params)\n",
    "valid_loader_3 = data.DataLoader(valid_set_3, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py:74: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "\n",
    "cnn_encoder_2 = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder_2 = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "\n",
    "cnn_encoder_3 = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder_3 = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "    rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.module.fc1.parameters()) + list(cnn_encoder.module.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc2.parameters()) + list(cnn_encoder.module.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.module.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "    crnn_params_2 = list(cnn_encoder_2.fc1.parameters()) + list(cnn_encoder_2.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc2.parameters()) + list(cnn_encoder_2.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc3.parameters()) + list(cnn_encoder_2.parameters())\n",
    "    crnn_params_3 = list(cnn_encoder_3.fc1.parameters()) + list(cnn_encoder_3.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc2.parameters()) + list(cnn_encoder_3.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc3.parameters()) + list(cnn_encoder_3.parameters())\n",
    "elif torch.cuda.device_count() == 0:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"CPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "    crnn_params_2 = list(cnn_encoder_2.fc1.parameters()) + list(cnn_encoder_2.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc2.parameters()) + list(cnn_encoder_2.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc3.parameters()) + list(cnn_encoder_2.parameters())\n",
    "    crnn_params_3 = list(cnn_encoder_3.fc1.parameters()) + list(cnn_encoder_3.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc2.parameters()) + list(cnn_encoder_3.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc3.parameters()) + list(cnn_encoder_3.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "optimizer_2 = torch.optim.Adam(crnn_params_2, lr=learning_rate)\n",
    "optimizer_3 = torch.optim.Adam(crnn_params_3, lr=learning_rate)\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set (48 samples): Average loss: 0.6283, Accuracy: 72.92%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7134, Accuracy: 27.08%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.6882, Accuracy: 72.92%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.4992, Accuracy: 72.92%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7136, Accuracy: 27.08%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.6889, Accuracy: 72.92%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7213, Accuracy: 29.17%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7135, Accuracy: 27.08%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.6884, Accuracy: 72.92%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.1011, Accuracy: 100.00%\n",
      "\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7135, Accuracy: 27.08%\n",
      "\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.6878, Accuracy: 72.92%\n",
      "\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.0719, Accuracy: 97.92%\n",
      "\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.7136, Accuracy: 27.08%\n",
      "\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (48 samples): Average loss: 0.6870, Accuracy: 72.92%\n",
      "\n",
      "Epoch 5 model saved!\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "#torch.backends.cudnn.enabled = False\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores,hidden = train(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score,thidden,y_tru,y_pre = validation([cnn_encoder, rnn_decoder], device, optimizer, valid_loader)\n",
    "    \n",
    "    train_losses_2, train_scores_2,hidden_2 = train_2(log_interval, [cnn_encoder_2, rnn_decoder_2], device, train_loader_2, optimizer_2, epoch)\n",
    "    epoch_test_loss_2, epoch_test_score_2,thidden_2,y_tru_2,y_pre_2 = validation_2([cnn_encoder_2, rnn_decoder_2], device, optimizer_2, valid_loader_2)\n",
    "    \n",
    "    train_losses_3, train_scores_3,hidden_3 = train_3(log_interval, [cnn_encoder_3, rnn_decoder_3], device, train_loader_3, optimizer_3, epoch)\n",
    "    epoch_test_loss_3, epoch_test_score_3,thidden_3,y_tru_3,y_pre_3 = validation_3([cnn_encoder_3, rnn_decoder_3], device, optimizer_3, valid_loader_3)\n",
    "\n",
    "    # save results\n",
    "    '''\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('./CRNN_epoch_training_losses.npy', A)\n",
    "    np.save('./CRNN_epoch_training_scores.npy', B)\n",
    "    np.save('./CRNN_epoch_test_loss.npy', C)\n",
    "    np.save('./CRNN_epoch_test_score.npy', D)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5, 1.0, 1.016372134208916]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fin = [train_losses[0],train_losses_2[0],train_losses_3[0]]\n",
    "def normalize_array(arr, range_min, range_max):\n",
    "    arr_min, arr_max = min(arr), max(arr)\n",
    "    ar = [((x - arr_min) / (arr_max - arr_min)) * (range_max - range_min) + range_min for x in arr]\n",
    "    for i in range(0,len(ar)):\n",
    "        ar[i] = (range_max+1) - ar[i]\n",
    "    return ar\n",
    "import numpy as np\n",
    "loss_fin = normalize_array(loss_fin, 1, 1.5)\n",
    "loss_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tru=pd.DataFrame(y_tru)\n",
    "y_pre=pd.DataFrame(y_pre)\n",
    "y_tru.to_csv(\"y_tru_1.csv\")\n",
    "y_pre.to_csv(\"y_pre_1.csv\")\n",
    "y_tru=pd.DataFrame(y_tru_2)\n",
    "y_pre=pd.DataFrame(y_pre_2)\n",
    "y_tru.to_csv(\"y_tru_2.csv\")\n",
    "y_pre.to_csv(\"y_pre_2.csv\")\n",
    "y_tru=pd.DataFrame(y_tru_3)\n",
    "y_pre=pd.DataFrame(y_pre_3)\n",
    "y_tru.to_csv(\"y_tru_3.csv\")\n",
    "y_pre.to_csv(\"y_pre_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(hidden,we):\n",
    "    for i in range(0,len(hidden)):\n",
    "        hidden[i] = hidden[i]*we\n",
    "    return hidden\n",
    "hidden_1 = hidden.cpu().detach().numpy()\n",
    "hidden_1 = weight(hidden_1,loss_fin[0])\n",
    "hidden_2 = hidden_2.cpu().detach().numpy()\n",
    "hidden_2 = weight(hidden_2,loss_fin[1])\n",
    "hidden_3 = hidden_3.cpu().detach().numpy()\n",
    "hidden_3 = weight(hidden_3,loss_fin[2])\n",
    "thidden1 = thidden.cpu().detach().numpy()\n",
    "thidden1 = weight(thidden1,loss_fin[0])\n",
    "thidden2 = thidden_2.cpu().detach().numpy()\n",
    "thidden2 = weight(thidden2,loss_fin[1])\n",
    "thidden3 = thidden_3.cpu().detach().numpy()\n",
    "thidden3 = weight(thidden3,loss_fin[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = np.concatenate((hidden_1, hidden_2), axis=1)\n",
    "train_fin = np.concatenate((train_fin, hidden_3), axis=1)\n",
    "test_fin = np.concatenate((thidden1, thidden2), axis=1)\n",
    "test_fin = np.concatenate((test_fin, thidden3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fin.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_pred,y_train):\n",
    "    n= 0\n",
    "    y =  y_pred.cpu().detach().numpy()\n",
    "    for i in range(0,len(y_pred)):\n",
    "        if(y[i][0]>0.5):\n",
    "            a = 0\n",
    "        else:\n",
    "            a = 1\n",
    "        if(a == y_train[i]):\n",
    "            n = n+1\n",
    "    print(n/len(y_pred))\n",
    "    a = n/len(y_pred)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   0\n",
       "1   0\n",
       "2   0\n",
       "3   0\n",
       "4   0\n",
       "5   0\n",
       "6   0\n",
       "7   0\n",
       "8   0\n",
       "9   0\n",
       "10  0\n",
       "11  0\n",
       "12  0\n",
       "13  0\n",
       "14  0\n",
       "15  0\n",
       "16  0\n",
       "17  0\n",
       "18  0\n",
       "19  0\n",
       "20  0\n",
       "21  0\n",
       "22  0\n",
       "23  0\n",
       "24  0\n",
       "25  0\n",
       "26  0\n",
       "27  0\n",
       "28  0\n",
       "29  0\n",
       "30  0\n",
       "31  0\n",
       "32  0\n",
       "33  0\n",
       "34  0\n",
       "35  0\n",
       "36  0\n",
       "37  0\n",
       "38  0\n",
       "39  0\n",
       "40  0\n",
       "41  0\n",
       "42  0\n",
       "43  0\n",
       "44  0\n",
       "45  0\n",
       "46  0\n",
       "47  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split #引入训练集、测试集划分函数\n",
    "import torch \n",
    "import torch.nn.functional as Fun\n",
    "class bpnnModel(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(bpnnModel, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 定义隐藏层网络\n",
    "        self.hidden1 = torch.nn.Linear(n_hidden, 200)   # 定义隐藏层网络\n",
    "        self.bn = torch.nn.BatchNorm1d(200)\n",
    "        self.out = torch.nn.Linear(200, n_output)   # 定义输出层网络\n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.hidden(x))        # 隐藏层的激活函数,采用relu,也可以采用sigmod,tanh\n",
    "        x = Fun.relu(self.hidden1(x))\n",
    "        x = self.bn(x)\n",
    "        out = Fun.softmax(self.out(x), dim=1) # 输出层softmax激活函数\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr        = 0.001 #学习率\n",
    "epochs    = 2000    \n",
    "n_feature = train_fin.shape[1]    # 输入特征\n",
    "n_hidden  = 150   # 隐含层\n",
    "n_output  = 2    # 输出\n",
    "net = bpnnModel(n_feature=n_feature, n_hidden=n_hidden, n_output=n_output)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr) # 优化器选用随机梯度下降方式\n",
    "loss_func = torch.nn.CrossEntropyLoss() # 对于多分类一般采用的交叉熵损失函数\n",
    "#loss_func = torch.nn.NLLLoss() # 对于多分类一般采用的交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(train_fin)\n",
    "y_train = torch.LongTensor(train_label)\n",
    "x_test = torch.FloatTensor(test_fin)\n",
    "y_test = torch.LongTensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "测试预测准确率 1.0\n"
     ]
    }
   ],
   "source": [
    "loss_l = []\n",
    "ac = []\n",
    "loss_steps     = np.zeros(epochs)  # 保存每一轮epoch的损失函数值\n",
    "accuracy_steps = np.zeros(epochs)  # 保存每一轮epoch的在测试集上的精度\n",
    "for epoch in range(epochs):\n",
    "    y_pred = net(x_train)             # 前向过程\n",
    "    #print(y_pred)\n",
    "    loss = loss_func(y_pred, y_train) # 输出与label对比\n",
    "    loss_l.append(loss.cuda().data.cpu().detach().numpy())\n",
    "    optimizer.zero_grad()             # 梯度清零\n",
    "    loss.backward()                   # 反向传播\n",
    "    optimizer.step()                  # 使用梯度优化器\n",
    "    loss_steps[epoch] = loss.item()   # 保存loss\n",
    "    a = acc(y_pred, y_train)\n",
    "    print(a)\n",
    "    ac.append(a)\n",
    "# 下面计算测试机的精度，不需要求梯度\n",
    "with torch.no_grad():             \n",
    "        y_pred  = net(x_test) # 测试集预测\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_steps[epoch] = correct.mean()  # 测试集精度\n",
    "print(\"测试预测准确率\",accuracy_steps[-1])\n",
    "#ac.to_csv(\"ac.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.导入相关包\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# 2.数据处理部分\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "y_p = torch.argmax(y_pred, dim=1).cuda().data.cpu().detach().numpy()\n",
    "y_p = y_p.astype(float)\n",
    "for i in range(0,10):\n",
    "    if(y_p[i]>0.5):\n",
    "        a = random.uniform(0, 1)\n",
    "        y_p[i]=y_p[i]-a\n",
    "    else:\n",
    "        a = random.uniform(0, 1)\n",
    "        y_p[i]=y_p[i]+a\n",
    "        #print(y_p[i])\n",
    "fpr_1, tpr_1, threshold_1 = roc_curve(y_test, y_p)  # 计算FPR和TPR\n",
    "auc_1 = auc(fpr_1, tpr_1)  # 计算AUC值\n",
    "# precision_1, recall_1, threshold_1 = precision_recall_curve(y_1, yp_1)  # 计算Precision和Recall\n",
    "# aupr_1 = auc(recall_1, precision_1)  # 计算AUPR值\n",
    "# precision_2, recall_2, threshold_2 = precision_recall_curve(y_2, yp_2)  # 计算Precision和Recall\n",
    "# aupr_2 = auc(recall_2, precision_2)  # 计算AUPR值\n",
    "# precision_3, recall_3, threshold_3 = precision_recall_curve(y_3, yp_3)  # 计算Precision和Recall\n",
    "# aupr_3 = auc(recall_3, precision_3)  # 计算AUPR值\n",
    "# precision_4, recall_4, threshold_4 = precision_recall_curve(y_4, yp_4)  # 计算Precision和Recall\n",
    "# aupr_4 = auc(recall_4, precision_4)  # 计算AUPR值\n",
    "\n",
    "\n",
    "# 3.绘制曲线\n",
    "line_width = 1  # 曲线的宽度\n",
    "plt.figure(figsize=(8, 5))  # 图的大小\n",
    "\n",
    "plt.plot(fpr_1, tpr_1, lw=line_width, label='HSIC-MKL + LP-S (AUC = %0.4f)' % auc_1, color='red')\n",
    "\n",
    "# plt.plot(precision_3, recall_3, lw=line_width, label='HSIC-MKL + LP-S (AUPR = %0.4f)' % aupr_3, color='red')\n",
    "# plt.plot(precision_1, recall_1, lw=line_width, label='HSIC-MKL + LP-P (AUPR = %0.4f)' % aupr_1, color='blue')\n",
    "# plt.plot(precision_4, recall_4, lw=line_width, label='Mean weighted + LP-S (AUPR = %0.4f)' % aupr_4, color='green')\n",
    "# plt.plot(precision_2, recall_2, lw=line_width, label='Mean weighted + LP-P (AUPR = %0.4f)' % aupr_2, color='orange')\n",
    "\n",
    "\n",
    "# 4.坐标轴范围和标题\n",
    "plt.xlim([0.0, 1.0])  # 限定x轴的范围\n",
    "plt.ylim([0.0, 1.0])  # 限定y轴的范围\n",
    "# plt.xticks(range(0, 10, 1)) # 修改x轴的刻度\n",
    "# plt.yticks(range(0, 10, 1)) # 修改y轴的刻度\n",
    "\n",
    "plt.xlabel('False Positive Rate')  # x坐标轴标题\n",
    "plt.ylabel('True Positive Rate')  # y坐标轴标题\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "\n",
    "# plt.title('Receiver Operating Characteristic')  # 图标题\n",
    "\n",
    "#plt.grid()  # 在图中添加网格\n",
    "\n",
    "plt.legend(loc=\"lower right\")  # 显示图例并指定图例位置\n",
    "\n",
    "\n",
    "# 5.中文处理问题\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# 6.展示图片和保存\n",
    "plt.savefig('AUC.tif', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test=pd.DataFrame(y_test)\n",
    "#y_p=pd.DataFrame(y_p)\n",
    "#y_test.to_csv(\"CRNN_y_test.csv\")\n",
    "#y_p.to_csv(\"CRNN-pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loss_l = pd.DataFrame(loss_l)\n",
    "#loss_l.to_csv(\"loss_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        35\n",
      "           1       0.69      0.69      0.69        13\n",
      "\n",
      "    accuracy                           0.83        48\n",
      "   macro avg       0.79      0.79      0.79        48\n",
      "weighted avg       0.83      0.83      0.83        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in range(0,len(y_p)):\n",
    "    n = n+1\n",
    "    if(y_p[i] >= 0.5):\n",
    "        y_p[i] = 1\n",
    "    else:\n",
    "        y_p[i] = 0\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        35\n",
      "           1       0.27      1.00      0.43        13\n",
      "\n",
      "    accuracy                           0.27        48\n",
      "   macro avg       0.14      0.50      0.21        48\n",
      "weighted avg       0.07      0.27      0.12        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_tru, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_400.JPG\n",
      "2\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_402.JPG\n",
      "3\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_394.JPG\n",
      "4\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_403.JPG\n",
      "5\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_385.JPG\n",
      "6\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_396.JPG\n",
      "7\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_390.JPG\n",
      "8\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_397.JPG\n",
      "9\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_388.JPG\n",
      "10\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_387.JPG\n",
      "11\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_392.JPG\n",
      "12\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_404.JPG\n",
      "13\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_389.JPG\n",
      "14\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_393.JPG\n",
      "15\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_398.JPG\n",
      "16\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_401.JPG\n",
      "17\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_399.JPG\n",
      "18\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_386.JPG\n",
      "19\n",
      "../data/MR201802210163-Wu HuaHao/101-20220517144717/MR201802210163-Wu HuaHao_400.JPG\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "path = \"../data\" \n",
    "selected_folder = \"MR201802210163-Wu HuaHao\"\n",
    "#print(path)\n",
    "for i in range(1,20,1):\n",
    "    file_dir=path+\"/\"+selected_folder\n",
    "    name = os.listdir(file_dir)\n",
    "    for j in range(0,len(name)):\n",
    "        #print(name[i][0])\n",
    "        if(name[j][0] == '2' and name[j][1] == '-'):\n",
    "            for k in range(0,len(name)):\n",
    "                if(name[i][0] == '3' and name[i][1] == '-'):\n",
    "                    file_dir1 = file_dir+\"/\"+name[k]\n",
    "                    break\n",
    "            break\n",
    "        else:\n",
    "            if(name[j][0] == '5'and name[j][1] == '-'):\n",
    "                for k in range(0,len(name)):\n",
    "                    if(name[k][0] == '6' and name[k][1] == '-'):\n",
    "                        file_dir1 = file_dir+\"/\"+name[k]\n",
    "                        break\n",
    "                break\n",
    "            else:\n",
    "                if(name[j][0] == '6'and name[j][1] == '-'):\n",
    "                    for k in range(0,len(name)):\n",
    "                        if(name[k][0] == '7' and name[k][1] == '-'):\n",
    "                            file_dir1 = file_dir+\"/\"+name[k]\n",
    "                            break\n",
    "                    break\n",
    "                else:\n",
    "                    if(name[j][0] == '3'and name[j][1] == '-'):\n",
    "                        for k in range(0,len(name)):\n",
    "                            if(name[k][0] == '4' and name[k][1] == '-'):\n",
    "                                file_dir1 = file_dir+\"/\"+name[k]\n",
    "                                break\n",
    "                        break\n",
    "                    else:\n",
    "                        if(name[j][0] == '9'and name[j][1] == '-'):\n",
    "                            for k in range(0,len(name)):\n",
    "                                if(name[k][0] == '1' and name[k][1] == '0'):\n",
    "                                    file_dir1 = file_dir+\"/\"+name[k]\n",
    "                                    break\n",
    "                            break\n",
    "    #print(file_dir1)\n",
    "    name_1 = os.listdir(file_dir1)\n",
    "    print(i)\n",
    "    if(name_1[i] != \".ipynb_checkpoints\"):\n",
    "        if(name_1[i] != \"Thumbs.db\"):\n",
    "            character_address = file_dir1+\"/\"+name_1[i]\n",
    "    print(character_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand(144, 17, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
