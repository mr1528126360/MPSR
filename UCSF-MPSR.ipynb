{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "data = pd.read_csv(\"dataname.csv\",sep=\",\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WHO CNS Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCSF-PDGM-232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCSF-PDGM-233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCSF-PDGM-234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCSF-PDGM-235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCSF-PDGM-236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>UCSF-PDGM-061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>UCSF-PDGM-063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>UCSF-PDGM-064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>UCSF-PDGM-065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>UCSF-PDGM-066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  WHO CNS Grade\n",
       "0    UCSF-PDGM-232              0\n",
       "1    UCSF-PDGM-233              0\n",
       "2    UCSF-PDGM-234              0\n",
       "3    UCSF-PDGM-235              0\n",
       "4    UCSF-PDGM-236              0\n",
       "..             ...            ...\n",
       "107  UCSF-PDGM-061              1\n",
       "108  UCSF-PDGM-063              1\n",
       "109  UCSF-PDGM-064              1\n",
       "110  UCSF-PDGM-065              1\n",
       "111  UCSF-PDGM-066              1\n",
       "\n",
       "[112 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCSF-PDGM-0232\n",
      "UCSF-PDGM-0233\n",
      "UCSF-PDGM-0234\n",
      "UCSF-PDGM-0235\n",
      "UCSF-PDGM-0236\n",
      "UCSF-PDGM-0237\n",
      "UCSF-PDGM-0238\n",
      "UCSF-PDGM-0239\n",
      "UCSF-PDGM-0240\n",
      "UCSF-PDGM-0242\n",
      "UCSF-PDGM-0244\n",
      "UCSF-PDGM-0245\n",
      "UCSF-PDGM-0246\n",
      "UCSF-PDGM-0247\n",
      "UCSF-PDGM-0248\n",
      "UCSF-PDGM-0250\n",
      "UCSF-PDGM-0252\n",
      "UCSF-PDGM-0253\n",
      "UCSF-PDGM-0255\n",
      "UCSF-PDGM-0256\n",
      "UCSF-PDGM-0257\n",
      "UCSF-PDGM-0258\n",
      "UCSF-PDGM-0259\n",
      "UCSF-PDGM-0260\n",
      "UCSF-PDGM-0261\n",
      "UCSF-PDGM-0264\n",
      "UCSF-PDGM-0265\n",
      "UCSF-PDGM-0266\n",
      "UCSF-PDGM-0267\n",
      "UCSF-PDGM-0269\n",
      "UCSF-PDGM-0281\n",
      "UCSF-PDGM-0286\n",
      "UCSF-PDGM-0300\n",
      "UCSF-PDGM-0302\n",
      "UCSF-PDGM-0305\n",
      "UCSF-PDGM-0307\n",
      "UCSF-PDGM-0308\n",
      "UCSF-PDGM-0327\n",
      "UCSF-PDGM-0351\n",
      "UCSF-PDGM-0438\n",
      "UCSF-PDGM-0439\n",
      "UCSF-PDGM-0442\n",
      "UCSF-PDGM-0443\n",
      "UCSF-PDGM-0444\n",
      "UCSF-PDGM-0446\n",
      "UCSF-PDGM-0448\n",
      "UCSF-PDGM-0456\n",
      "UCSF-PDGM-0465\n",
      "UCSF-PDGM-0475\n",
      "UCSF-PDGM-0476\n",
      "UCSF-PDGM-0477\n",
      "UCSF-PDGM-0478\n",
      "UCSF-PDGM-0483\n",
      "UCSF-PDGM-0485\n",
      "UCSF-PDGM-0490\n",
      "UCSF-PDGM-0540\n",
      "UCSF-PDGM-0004\n",
      "UCSF-PDGM-0005\n",
      "UCSF-PDGM-0007\n",
      "UCSF-PDGM-0008\n",
      "UCSF-PDGM-0009\n",
      "UCSF-PDGM-0010\n",
      "UCSF-PDGM-0011\n",
      "UCSF-PDGM-0012\n",
      "UCSF-PDGM-0013\n",
      "UCSF-PDGM-0014\n",
      "UCSF-PDGM-0015\n",
      "UCSF-PDGM-0016\n",
      "UCSF-PDGM-0017\n",
      "UCSF-PDGM-0018\n",
      "UCSF-PDGM-0019\n",
      "UCSF-PDGM-0020\n",
      "UCSF-PDGM-0021\n",
      "UCSF-PDGM-0022\n",
      "UCSF-PDGM-0023\n",
      "UCSF-PDGM-0024\n",
      "UCSF-PDGM-0025\n",
      "UCSF-PDGM-0026\n",
      "UCSF-PDGM-0027\n",
      "UCSF-PDGM-0029\n",
      "UCSF-PDGM-0030\n",
      "UCSF-PDGM-0031\n",
      "UCSF-PDGM-0032\n",
      "UCSF-PDGM-0033\n",
      "UCSF-PDGM-0034\n",
      "UCSF-PDGM-0035\n",
      "UCSF-PDGM-0036\n",
      "UCSF-PDGM-0037\n",
      "UCSF-PDGM-0038\n",
      "UCSF-PDGM-0039\n",
      "UCSF-PDGM-0040\n",
      "UCSF-PDGM-0041\n",
      "UCSF-PDGM-0042\n",
      "UCSF-PDGM-0043\n",
      "UCSF-PDGM-0044\n",
      "UCSF-PDGM-0045\n",
      "UCSF-PDGM-0046\n",
      "UCSF-PDGM-0047\n",
      "UCSF-PDGM-0048\n",
      "UCSF-PDGM-0049\n",
      "UCSF-PDGM-0050\n",
      "UCSF-PDGM-0053\n",
      "UCSF-PDGM-0055\n",
      "UCSF-PDGM-0056\n",
      "UCSF-PDGM-0057\n",
      "UCSF-PDGM-0058\n",
      "UCSF-PDGM-0059\n",
      "UCSF-PDGM-0061\n",
      "UCSF-PDGM-0063\n",
      "UCSF-PDGM-0064\n",
      "UCSF-PDGM-0065\n",
      "UCSF-PDGM-0066\n"
     ]
    }
   ],
   "source": [
    "all_names = []\n",
    "path_label=[]\n",
    "for h in range(0,len(data[\"ID\"])):\n",
    "    file_dir=data[\"ID\"][h]\n",
    "    original = data[\"ID\"][h]\n",
    "    formatted = f\"{original[:-3]:}{int(original.split('-')[-1]):04}\"\n",
    "    file_dir = formatted\n",
    "    print(file_dir)\n",
    "    all_names.append(file_dir)\n",
    "    path_label.append(data[\"WHO CNS Grade\"][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_list = all_names                  # all video file names\n",
    "all_y_list = np.array(path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_name_path = './UCF101actions.pkl'\n",
    "save_model_path = \"./ResNetCRNN_ckpt/\"\n",
    "data_path = \"./dataset\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.1       # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 2             # number of target category\n",
    "epochs = 50       # training epochs\n",
    "batch_size = 100  #这要设置成和训练数据集一样的常度\n",
    "learning_rate = 1e-3\n",
    "log_interval = 5   # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 20, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "encoder_layer2 = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=2)\n",
    "encoder_layer3 = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder3 = nn.TransformerEncoder(encoder_layer3, num_layers=2)\n",
    "def normalize_list(lst):\n",
    "    np_list = np.array(lst)\n",
    "    normalized_list = (np_list - np_list.min()) / (np_list.max() - np_list.min()) + 1\n",
    "    return normalized_list.tolist()\n",
    "def L1_tensor(x_feature):\n",
    "    for i in range(0, len(x_feature)):\n",
    "        #print(x_feature[i].shape)\n",
    "        list_1 = []\n",
    "        ls = x_feature[i]\n",
    "        for j in ls:\n",
    "            cc = np.sum(j)\n",
    "            cc = abs(cc)\n",
    "            list_1.append(cc)\n",
    "            # print(cc)\n",
    "        list_2 = normalize_list(list_1)\n",
    "        # print(list_2)\n",
    "        for k in range(0, len(x_feature[i])):\n",
    "            x_feature[i][k] = x_feature[i][k] * list_2[k]\n",
    "    return  x_feature\n",
    "if torch.cuda.is_available():\n",
    "    transformer_encoder = transformer_encoder.to('cuda:0')\n",
    "    transformer_encoder2 = transformer_encoder2.to('cuda:0')\n",
    "    transformer_encoder3 = transformer_encoder3.to('cuda:0')\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()       \n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracyLo\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        #print(y.cpu().data.squeeze().numpy())\n",
    "        #print(\"真实值\")\n",
    "        #print(y_pred.cpu().data.squeeze().numpy())\n",
    "        #print(\"预测值\")\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "    #print(all_y.cpu().data.squeeze().numpy())\n",
    "    #print(\"真实值\")\n",
    "    #print(all_y_pred.cpu().data.squeeze().numpy())\n",
    "    #print(\"预测值\")\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder2(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation_2(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder2(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out_1.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "    \n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src = cnn_encoder(X)\n",
    "        out = transformer_encoder3(src)\n",
    "        x_f = out.cpu().data.numpy()\n",
    "        x_f = L1_tensor(x_f)\n",
    "        x_f = torch.FloatTensor(x_f)\n",
    "        out_1 = x_f.to('cuda')\n",
    "        #print(out_1.shape)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "        #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores,hidden\n",
    "\n",
    "\n",
    "def validation_3(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            src = cnn_encoder(X)\n",
    "            out = transformer_encoder3(src)\n",
    "            x_f = out.cpu().data.numpy()\n",
    "            x_f = L1_tensor(x_f)\n",
    "            x_f = torch.FloatTensor(x_f)\n",
    "            out_1 = x_f.to('cuda')\n",
    "            #print(out_1.shape)\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "            output,hidden = rnn_decoder(out_1)   # output has dim = (batch, number of classes)\n",
    "\n",
    "            #output,hidden = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}'.format(len(all_y), test_loss))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    y_tru = all_y.cpu().data.squeeze().numpy()\n",
    "    y_pre = all_y_pred.cpu().data.squeeze().numpy()\n",
    "    return test_loss, test_score,hidden,y_tru,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 2, 'pin_memory': True} if use_cuda else {'batch_size': batch_size}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split\n",
    "train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.2)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "train_set, valid_set = Dataset_CRNN(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_set_2, valid_set_2 = Dataset_CRNN_2(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN_2(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_set_3, valid_set_3 = Dataset_CRNN_3(data_path, train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_CRNN_3(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)\n",
    "\n",
    "train_loader_2 = data.DataLoader(train_set_2, **params)\n",
    "valid_loader_2 = data.DataLoader(valid_set_2, **params)\n",
    "\n",
    "train_loader_3 = data.DataLoader(train_set_3, **params)\n",
    "valid_loader_3 = data.DataLoader(valid_set_3, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15281\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:74: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "\n",
    "cnn_encoder_2 = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder_2 = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "\n",
    "cnn_encoder_3 = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "rnn_decoder_3 = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "    crnn_params_2 = list(cnn_encoder_2.fc1.parameters()) + list(cnn_encoder_2.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc2.parameters()) + list(cnn_encoder_2.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc3.parameters()) + list(cnn_encoder_2.parameters())\n",
    "    crnn_params_3 = list(cnn_encoder_3.fc1.parameters()) + list(cnn_encoder_3.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc2.parameters()) + list(cnn_encoder_3.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc3.parameters()) + list(cnn_encoder_3.parameters())\n",
    "elif torch.cuda.device_count() == 0:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"CPU!\")\n",
    "    # Combine all EncoderCNN + DecoderRNN parameters\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "    crnn_params_2 = list(cnn_encoder_2.fc1.parameters()) + list(cnn_encoder_2.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc2.parameters()) + list(cnn_encoder_2.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_2.fc3.parameters()) + list(cnn_encoder_2.parameters())\n",
    "    crnn_params_3 = list(cnn_encoder_3.fc1.parameters()) + list(cnn_encoder_3.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc2.parameters()) + list(cnn_encoder_3.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder_3.fc3.parameters()) + list(cnn_encoder_3.parameters())\n",
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "optimizer_2 = torch.optim.Adam(crnn_params_2, lr=learning_rate)\n",
    "optimizer_3 = torch.optim.Adam(crnn_params_3, lr=learning_rate)\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set (23 samples): Average loss: 0.6944\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6967\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6859\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.7378\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6959\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6855\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 1.2892\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6956\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6852\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 2.5416\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6956\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6853\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 1.1273\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6951\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6841\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 1.2961\n",
      "Epoch 6 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6959\n",
      "Epoch 6 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6842\n",
      "Epoch 6 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6431\n",
      "Epoch 7 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 7 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6842\n",
      "Epoch 7 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3601\n",
      "Epoch 8 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6959\n",
      "Epoch 8 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6843\n",
      "Epoch 8 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2515\n",
      "Epoch 9 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 9 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6843\n",
      "Epoch 9 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2678\n",
      "Epoch 10 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 10 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6844\n",
      "Epoch 10 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2985\n",
      "Epoch 11 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 11 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6844\n",
      "Epoch 11 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3238\n",
      "Epoch 12 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6959\n",
      "Epoch 12 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6844\n",
      "Epoch 12 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3330\n",
      "Epoch 13 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 13 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6846\n",
      "Epoch 13 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3280\n",
      "Epoch 14 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6959\n",
      "Epoch 14 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6845\n",
      "Epoch 14 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3116\n",
      "Epoch 15 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 15 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6847\n",
      "Epoch 15 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2930\n",
      "Epoch 16 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 16 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6847\n",
      "Epoch 16 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2768\n",
      "Epoch 17 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 17 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6845\n",
      "Epoch 17 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2661\n",
      "Epoch 18 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 18 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6847\n",
      "Epoch 18 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2603\n",
      "Epoch 19 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 19 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 19 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2588\n",
      "Epoch 20 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 20 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6847\n",
      "Epoch 20 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2595\n",
      "Epoch 21 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6958\n",
      "Epoch 21 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 21 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2649\n",
      "Epoch 22 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 22 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 22 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2751\n",
      "Epoch 23 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 23 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 23 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2870\n",
      "Epoch 24 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 24 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 24 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2975\n",
      "Epoch 25 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 25 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 25 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3031\n",
      "Epoch 26 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 26 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 26 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3039\n",
      "Epoch 27 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 27 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6849\n",
      "Epoch 27 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3006\n",
      "Epoch 28 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 28 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6846\n",
      "Epoch 28 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2912\n",
      "Epoch 29 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 29 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6847\n",
      "Epoch 29 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2809\n",
      "Epoch 30 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 30 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 30 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2726\n",
      "Epoch 31 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 31 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 31 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2699\n",
      "Epoch 32 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6960\n",
      "Epoch 32 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 32 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2677\n",
      "Epoch 33 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 33 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6849\n",
      "Epoch 33 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2708\n",
      "Epoch 34 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 34 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 34 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2715\n",
      "Epoch 35 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 35 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 35 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2724\n",
      "Epoch 36 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 36 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6849\n",
      "Epoch 36 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.2855\n",
      "Epoch 37 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 37 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 37 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3033\n",
      "Epoch 38 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 38 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6849\n",
      "Epoch 38 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3005\n",
      "Epoch 39 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6963\n",
      "Epoch 39 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6852\n",
      "Epoch 39 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.3623\n",
      "Epoch 40 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 40 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6851\n",
      "Epoch 40 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.4197\n",
      "Epoch 41 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6963\n",
      "Epoch 41 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6849\n",
      "Epoch 41 model saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set (23 samples): Average loss: 0.4658\n",
      "Epoch 42 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 42 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6851\n",
      "Epoch 42 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.5142\n",
      "Epoch 43 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6961\n",
      "Epoch 43 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6851\n",
      "Epoch 43 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.5640\n",
      "Epoch 44 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6964\n",
      "Epoch 44 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 44 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6120\n",
      "Epoch 45 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 45 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6852\n",
      "Epoch 45 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6238\n",
      "Epoch 46 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6963\n",
      "Epoch 46 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 46 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6663\n",
      "Epoch 47 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6965\n",
      "Epoch 47 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 47 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.7029\n",
      "Epoch 48 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6962\n",
      "Epoch 48 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6848\n",
      "Epoch 48 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.4012\n",
      "Epoch 49 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6963\n",
      "Epoch 49 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 49 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.4250\n",
      "Epoch 50 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6963\n",
      "Epoch 50 model saved!\n",
      "\n",
      "Test set (23 samples): Average loss: 0.6850\n",
      "Epoch 50 model saved!\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "#torch.backends.cudnn.enabled = False\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores,hidden = train(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score,thidden,y_tru,y_pre = validation([cnn_encoder, rnn_decoder], device, optimizer, valid_loader)\n",
    "    \n",
    "    train_losses_2, train_scores_2,hidden_2 = train_2(log_interval, [cnn_encoder_2, rnn_decoder_2], device, train_loader_2, optimizer_2, epoch)\n",
    "    epoch_test_loss_2, epoch_test_score_2,thidden_2,y_tru_2,y_pre_2 = validation_2([cnn_encoder_2, rnn_decoder_2], device, optimizer_2, valid_loader_2)\n",
    "    \n",
    "    train_losses_3, train_scores_3,hidden_3 = train_3(log_interval, [cnn_encoder_3, rnn_decoder_3], device, train_loader_3, optimizer_3, epoch)\n",
    "    epoch_test_loss_3, epoch_test_score_3,thidden_3,y_tru_3,y_pre_3 = validation_3([cnn_encoder_3, rnn_decoder_3], device, optimizer_3, valid_loader_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2000000000000002, 1.000867403527144, 1.0000000000000002]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fin = [train_losses[0],train_losses_2[0],train_losses_3[0]]\n",
    "def normalize_array(arr, range_min, range_max):\n",
    "    arr_min, arr_max = min(arr), max(arr)\n",
    "    ar = [((x - arr_min) / (arr_max - arr_min)) * (range_max - range_min) + range_min for x in arr]\n",
    "    for i in range(0,len(ar)):\n",
    "        ar[i] = (range_max+1) - ar[i]\n",
    "    return ar\n",
    "import numpy as np\n",
    "loss_fin = normalize_array(loss_fin, 1, 1.2)\n",
    "loss_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(hidden,we):\n",
    "    for i in range(0,len(hidden)):\n",
    "        hidden[i] = hidden[i]*we\n",
    "    return hidden\n",
    "hidden_1 = hidden.cpu().detach().numpy()\n",
    "hidden_1 = weight(hidden_1,loss_fin[0])\n",
    "hidden_2 = hidden_2.cpu().detach().numpy()\n",
    "hidden_2 = weight(hidden_2,loss_fin[1])\n",
    "hidden_3 = hidden_3.cpu().detach().numpy()\n",
    "hidden_3 = weight(hidden_3,loss_fin[2])\n",
    "thidden1 = thidden.cpu().detach().numpy()\n",
    "thidden1 = weight(thidden1,loss_fin[0])\n",
    "thidden2 = thidden_2.cpu().detach().numpy()\n",
    "thidden2 = weight(thidden2,loss_fin[1])\n",
    "thidden3 = thidden_3.cpu().detach().numpy()\n",
    "thidden3 = weight(thidden3,loss_fin[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = np.concatenate((hidden_1, hidden_2), axis=1)\n",
    "train_fin = np.concatenate((train_fin, hidden_3), axis=1)\n",
    "test_fin = np.concatenate((thidden1, thidden2), axis=1)\n",
    "test_fin = np.concatenate((test_fin, thidden3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_pred,y_train):\n",
    "    n= 0\n",
    "    y =  y_pred.cpu().detach().numpy()\n",
    "    for i in range(0,len(y_pred)):\n",
    "        if(y[i][0]>0.5):\n",
    "            a = 0\n",
    "        else:\n",
    "            a = 1\n",
    "        if(a == y_train[i]):\n",
    "            n = n+1\n",
    "    #print(n/len(y_pred))\n",
    "    a = n/len(y_pred)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split #引入训练集、测试集划分函数\n",
    "import torch \n",
    "import torch.nn.functional as Fun\n",
    "class bpnnModel(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(bpnnModel, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 定义隐藏层网络\n",
    "        self.hidden1 = torch.nn.Linear(n_hidden, 200)   # 定义隐藏层网络\n",
    "        self.bn = torch.nn.BatchNorm1d(200)\n",
    "        self.out = torch.nn.Linear(200, n_output)   # 定义输出层网络\n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.hidden(x))        # 隐藏层的激活函数,采用relu,也可以采用sigmod,tanh\n",
    "        x = Fun.relu(self.hidden1(x))\n",
    "        x = self.bn(x)\n",
    "        out = Fun.softmax(self.out(x), dim=1) # 输出层softmax激活函数\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr        = 0.001 \n",
    "epochs    = 3000    \n",
    "n_feature = train_fin.shape[1] \n",
    "n_hidden  = 1000  \n",
    "n_output  = 2   \n",
    "net = bpnnModel(n_feature=n_feature, n_hidden=n_hidden, n_output=n_output)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(train_fin)\n",
    "y_train = torch.LongTensor(train_label)\n",
    "x_test = torch.FloatTensor(test_fin)\n",
    "y_test = torch.LongTensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011235955056179775\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "loss_l = []\n",
    "ac = []\n",
    "loss_steps     = np.zeros(epochs)\n",
    "accuracy_steps = np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    y_pred = net(x_train)\n",
    "    #print(y_pred)\n",
    "    loss = loss_func(y_pred, y_train)\n",
    "    loss_l.append(loss.cuda().data.cpu().detach().numpy())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_steps[epoch] = loss.item()\n",
    "    a = acc(y_pred, y_train)\n",
    "    if epoch%500 == 0:\n",
    "        print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试预测准确率 0.95652174949646\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred  = net(x_test) # 测试集预测\n",
    "    correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "    accuracy_steps[epoch] = correct.mean()  # 测试集精度\n",
    "    print(\"ACC\",accuracy_steps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        14\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.95      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, torch.argmax(y_pred, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
